{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4 Part 2: DQN for Cartpole Balance Task\n",
    "\n",
    "In the previous part, you have implemented the table methods for the cartpole using temporal difference (TD) learning and have some basic understandings of reinforcement learning. In this part, you will implement the Deep Q-Network (DQN) algorithm for the Cartpole Balance task. The DQN algorithm uses a neural network to approximate the Q-function, which allows us to learn a policy for balancing the pole on the cart. You will implement the DQN algorithm, train the agent using the Cartpole Balance task, and visualize the learning progress by plotting the total rewards obtained in each episode.\n",
    "\n",
    "Let's start with basic imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# if you did not install the requirements yet, uncomment the following lines to install them\n",
    "# !pip install torch\n",
    "# !pip install dm_control\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "\n",
    "# set up interactive matplotlib\n",
    "is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Ensure that you have the DeepMind Control Suite installed and set up with the CartPole environment. Your environment should provide:\n",
    "  - **State Observations:** Continuous values representing the pole angle, cart position, velocity, etc.\n",
    "  - **Discrete Actions:** For example, moving the cart left or right.\n",
    "  - **Reward Structure:** Define the reward for balancing the pole.\n",
    "  - **Hyperparameters:** Specify important hyperparameters such as learning rate, discount factor, and exploration strategy.\n",
    "\n",
    "This is provided in the `src.dqn.DMControlCartPoleWrapper` class, which wraps the DeepMind Control Suite CartPole environment to discretize the continuous state space and provide a reward structure suitable for the DQN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dqn import DMControlCartPoleWrapper\n",
    "\n",
    "env = DMControlCartPoleWrapper(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "frame = env.env.physics.render(height=400, width=400, camera_id=0)\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "Experience replay involves storing the agent’s experiences $(s, a, s', r)$ in a replay buffer. During training, random samples from this buffer are used to update the network. This has several advantages:\n",
    "\n",
    "- **Breaking Correlations:** Random sampling prevents the network from learning spurious correlations that arise from sequential data.\n",
    "- **Efficient Use of Data:** Each experience is used multiple times in training, making learning more efficient.\n",
    "\n",
    "The next cell demonstrates how to generate and record one transition and push it into the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dqn import Transition, ReplayMemory\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "obs, _ = env.reset()\n",
    "state_t = torch.tensor(obs, device=device, dtype=torch.float32).unsqueeze(0)\n",
    "action_t = torch.tensor([[1]], device=device, dtype=torch.long)\n",
    "next_obs, reward_val, term, trunc, _ = env.step(action_t.item())\n",
    "if not (term or trunc):\n",
    "    next_state_t = torch.tensor(next_obs, device=device, dtype=torch.float32).unsqueeze(\n",
    "        0\n",
    "    )\n",
    "else:\n",
    "    next_state_t = None\n",
    "\n",
    "reward_t = torch.tensor([reward_val], device=device, dtype=torch.float32)\n",
    "transition = Transition(state_t, action_t, next_state_t, reward_t)\n",
    "print(\"transition is \", transition)\n",
    "\n",
    "# push the transition to the replay memory\n",
    "replay = ReplayMemory(1000)\n",
    "replay.push(state_t, action_t, next_state_t, reward_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning basics\n",
    "\n",
    "In Q learning, we define a state-action value function,  $Q(s, a)$ , which estimates the expected return (sum of discounted rewards) when taking action  $a$  in state  $s$  and thereafter following the optimal policy. The Q function satisfies the Bellman optimality equation:\n",
    "\n",
    "\n",
    "$$\n",
    "Q(s, a) = \\mathbb{E}_{s'}\\left[r(s, a) + \\gamma \\max_{a'} Q(s', a')\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "- $r(s, a)$  is the immediate reward received after taking action  $a$  in state  $s$ ,\n",
    "- $s'$  is the next state,\n",
    "- $\\gamma \\in [0,1)$  is the discount factor that weighs future rewards,\n",
    "- $\\max_{a'} Q(s', a')$  represents the best possible future reward obtainable from state  $s'$ .\n",
    "\n",
    "\n",
    "### Tabular Q Learning Limitations\n",
    "\n",
    "In the traditional table-based Q learning:\n",
    "- **Scalability:** A table must be maintained for every state-action pair. For continuous or high-dimensional state spaces (like the one in CartPole), this table becomes enormous or even infinite. (as you might have explored and realized in the previous section)\n",
    "- **Generalization:** The table cannot generalize across similar states. If the agent encounters a state that’s not exactly in the table, it struggles to estimate a good Q value.\n",
    "\n",
    "\n",
    "## Deep Q Networks (DQNs)\n",
    "\n",
    "A DQN approximates the Q function using a neural network parameterized by weights  $\\theta$ . Instead of storing discrete Q-values in a table, the network learns a mapping:\n",
    "\n",
    "$$\n",
    "Q(s, a; \\theta) \\approx Q^*(s, a)\n",
    "$$\n",
    "\n",
    "where  $Q^*(s, a)$  is the optimal Q-function.\n",
    "\n",
    "This allows the agent to generalize from seen to unseen states, which is especially useful in environments with continuous state spaces.\n",
    "\n",
    "### Implementation of DQN in PyTorch\n",
    "\n",
    "You will implement a PyTorch module for a Deep Q Network (DQN) that maps a state observation to a set of Q-values—one for each possible action. Below is an in-depth description of the implementation:\n",
    "\n",
    "#### Network Architecture\n",
    "- **Input Layer:**\n",
    "The network accepts an input tensor with a size corresponding to the number of observations in the environment. This tensor represents the current state.\n",
    "- **Hidden Layers:**\n",
    "The model contains two fully connected (linear) hidden layers:\n",
    "    - The first hidden layer transforms the input from size n_observations to 128 features.\n",
    "    - The second hidden layer further processes these 128 features, maintaining the same dimensionality (128 units).\n",
    "Both hidden layers use the ReLU activation function, which introduces non-linearity and helps the network learn complex state-action value relationships.\n",
    "- **Output Layer:**\n",
    "The final layer is another fully connected layer that maps the 128 features to n_actions outputs. Each output corresponds to the Q-value associated with a particular action in the given state. No activation function is applied at this stage because the Q-values can range over all real numbers.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "The forward method defines how data flows through the network:\n",
    "1. Input Processing:\n",
    "The input tensor x is first passed through the first linear layer (layer1), followed by a ReLU activation.\n",
    "2. Intermediate Representation:\n",
    "The output of the first layer is then fed into the second linear layer (layer2) and again processed with a ReLU activation, ensuring that the network learns a rich feature representation.\n",
    "3. Output Computation:\n",
    "Finally, the transformed data is passed through the third linear layer (layer3), which produces the Q-values for each action. These values are used in the DQN algorithm to estimate the expected rewards for taking different actions in the current state.\n",
    "\n",
    "Mathematical Overview\n",
    "\n",
    "The computations performed by the network can be summarized as follows:\n",
    "1. First Hidden Layer:\n",
    "\n",
    "$$\n",
    "h_1 = \\text{ReLU}(W_1 x + b_1)\n",
    "$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{128 \\times n\\_observations}$ and $b_1 \\in \\mathbb{R}^{128}$.\n",
    "2. Second Hidden Layer:\n",
    "\n",
    "$$\n",
    "h_2 = \\text{ReLU}(W_2 h_1 + b_2)\n",
    "$$\n",
    "\n",
    "where $W_2 \\in \\mathbb{R}^{128 \\times 128}$ and $b_2 \\in \\mathbb{R}^{128}$.\n",
    "\n",
    "3. Output Layer:\n",
    "\n",
    "$$\n",
    "Q(x) = W_3 h_2 + b_3\n",
    "$$\n",
    "\n",
    "where $W_3 \\in \\mathbb{R}^{n\\_actions \\times 128}$ and $b_3 \\in \\mathbb{R}^{n\\_actions}$.\n",
    "\n",
    "These equations illustrate how the input state is successively transformed through linear mappings and nonlinear activations, culminating in a vector of Q-values used for decision making.\n",
    "\n",
    "Now you should implement the DQN model in the `src.dqn.DQN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dqn import DQN\n",
    "\n",
    "# create an example policy network\n",
    "policy_net = DQN(env.obs_size, env.action_space_n).to(device)\n",
    "print(policy_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQNTrainer: A Comprehensive Guide\n",
    "\n",
    "The DQNTrainer class encapsulates the training process for a Deep Q-Network (DQN) agent. This guide describes the mathematical foundations of the training process, the role of each function, and specific PyTorch implementation details such as device management, optimizers, and tensor handling.\n",
    "\n",
    "### Overview of the Training Process\n",
    "\n",
    "In reinforcement learning, the goal is to learn a policy that maximizes the expected cumulative reward. For Q-learning, we approximate the optimal action-value function  $Q^*(s, a)$ using the Bellman equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) = \\mathbb{E}{s{\\prime}}\\left[ r(s, a) + \\gamma \\max{a{\\prime}} Q(s{\\prime}, a{\\prime}) \\right]\n",
    "$$\n",
    "\n",
    "Deep Q-Networks (DQN) replace the table-based method with a neural network to approximate  $Q(s, a; \\theta)$ . The training objective is to minimize the difference between the predicted Q-values and target Q-values computed using a separate target network. This is formalized by the loss function:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}{(s, a, r, s{\\prime}) \\sim D} \\left[ \\left( r + \\gamma \\max{a{\\prime}} Q(s{\\prime}, a{\\prime}; \\theta^{-}) - Q(s, a; \\theta) \\right)^2 \\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta$  are the parameters of the current (policy) network.\n",
    "- $\\theta^{-}$  are the parameters of the target network.\n",
    "- $\\gamma$  is the discount factor.\n",
    "- $D$  is the experience replay buffer.\n",
    "\n",
    "To stabilize training, experience replay and soft target updates are employed. Experience replay allows the network to learn from a randomized batch of past experiences, while soft updates gradually adjust the target network towards the policy network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Structure and Function Descriptions\n",
    "\n",
    "#### 1. Initialization: `__init__`\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Sets up the training environment by initializing the policy and target networks, the optimizer, and the replay memory.\n",
    "\n",
    "**Key Operations:**\n",
    "\n",
    "- Networks:\n",
    "The policy network  $Q(s, a; \\theta)$  and target network  $Q(s, a; \\theta^{-})$  are created using the provided observation and action space sizes. The target network is initially synchronized with the policy network.\n",
    "\n",
    "- Optimizer:\n",
    "The AdamW optimizer is configured with learning rate `params.LR` and AMSGrad enabled. This is crucial for stable convergence.\n",
    "\n",
    "- Device Management:\n",
    "All networks are moved to the specified device (CPU or GPU) to leverage hardware acceleration.\n",
    "\n",
    "- Hyperparameters:\n",
    "Parameters such as maximum steps per episode and the number of episodes are stored along with additional training hyperparameters (e.g., learning rate, discount factor  $\\gamma$ , epsilon parameters for exploration).\n",
    "\n",
    "\n",
    "#### 2. Action Selection: `select_action`\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Chooses an action using an $\\epsilon$-greedy policy that balances exploration and exploitation.\n",
    "\n",
    "**Mathematical Framework:**\n",
    "\n",
    "- Epsilon Decay:\n",
    "\n",
    "The exploration rate $\\epsilon$ is decayed exponentially:\n",
    "\n",
    "$$\n",
    "\\epsilon = \\epsilon_{\\text{end}} + (\\epsilon_{\\text{start}} - \\epsilon_{\\text{end}}) \\exp\\left(-\\frac{\\text{steps done}}{\\epsilon_{\\text{decay}}}\\right)\n",
    "$$\n",
    "\n",
    "- Policy:\n",
    "\n",
    "With probability $(1 - \\epsilon)$, the action $a$ is selected as:\n",
    "\n",
    "$$\n",
    "a = \\arg\\max_{a{\\prime}} Q(s, a{\\prime}; \\theta)\n",
    "$$\n",
    "\n",
    "Otherwise, a random action is chosen.\n",
    "\n",
    "**PyTorch Specifics:**\n",
    "- The state is provided as a tensor.\n",
    "- No gradient is computed during the exploitation phase using torch.no_grad().\n",
    "- The selected action is returned as a tensor.\n",
    "\n",
    "\n",
    "#### 3. Model Optimization: `optimize_model`\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Performs one gradient descent update on the policy network using a randomly sampled minibatch from the replay memory.\n",
    "    \n",
    "**Steps in the Process:**\n",
    "\n",
    "1. Sampling the Batch:\n",
    "A minibatch of transitions  $(s, a, r, s{\\prime})$  is sampled from the replay memory.\n",
    "2. Batch Processing:\n",
    "The transitions are unpacked, and tensors for states, actions, rewards, and non-terminal next states are prepared.\n",
    "3. Q-value Calculation:\n",
    "- Current Q-values:\n",
    "For each state  $s$  in the batch:\n",
    "\n",
    "$Q(s, a; \\theta) = \\text{policy\\_net}(s)$  and we select $Q(s, a)$  via \\texttt{gather}\n",
    "\n",
    "- Next Q-values:\n",
    "\n",
    "For non-terminal states:\n",
    "\n",
    "$$\n",
    "\\max_{a{\\prime}} Q(s{\\prime}, a{\\prime}; \\theta^{-})\n",
    "$$\n",
    "\n",
    "Terminal states have a value of 0.\n",
    "\n",
    "4. Target Computation:\n",
    "The target Q-value is computed as:\n",
    "\n",
    "$$\n",
    "y = r + \\gamma \\max_{a{\\prime}} Q(s{\\prime}, a{\\prime}; \\theta^{-})\n",
    "$$\n",
    "\n",
    "5. Loss Calculation:\n",
    "The loss is computed using the Smooth L1 (Huber) loss:\n",
    "\n",
    "$$ \n",
    "L = \\text{SmoothL1Loss}\\left(Q(s, a; \\theta), y\\right)\n",
    "$$\n",
    "\n",
    "6. Gradient Descent:\n",
    "- Gradients are computed via backpropagation.\n",
    "- Gradient clipping is applied to avoid exploding gradients.\n",
    "- The optimizer steps to update the network parameters.\n",
    "\n",
    "**PyTorch Specifics:**\n",
    "\n",
    "- Use of `torch.no_grad()` ensures that target calculations do not accumulate gradients.\n",
    "- Tensors are carefully managed on the correct device.\n",
    "- `torch.nn.utils.clip_grad_value_` is used to clip gradients.\n",
    "\n",
    "\n",
    "#### 4. Target Network Update: `soft_update`\n",
    "    \n",
    "**Purpose:**\n",
    "\n",
    "Performs a soft update of the target network parameters to slowly track the policy network.\n",
    "\n",
    "**Mathematical Formulation:**\n",
    "\n",
    "- For each parameter $\\theta_{\\text{target}}$ in the target network:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{target}} \\leftarrow \\tau \\theta_{\\text{policy}} + (1 - \\tau) \\theta_{\\text{target}}\n",
    "$$\n",
    "\n",
    "where $\\tau$ is a small constant (e.g., 0.005) that determines the update rate. This gradual adjustment helps stabilize training.\n",
    "\n",
    "**Implementation Details:**\n",
    "\n",
    "- Both the target and policy network state dictionaries are iterated over, and the update is applied element-wise.\n",
    "- Ensure that the parameters are moved to the appropriate device (CPU/GPU) prior to the update.\n",
    "\n",
    "#### 5. Training Loop: `train`\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "Executes the main training loop across multiple episodes, integrating all components described above.\n",
    "\n",
    "**Training Process Overview:**\n",
    "\n",
    "1. Episode Initialization:\n",
    "- Reset the environment.\n",
    "- Convert the initial observation into a tensor on the correct device.\n",
    "2. Step Loop:\n",
    "- Action Selection:\n",
    "The agent selects an action using select_action.\n",
    "    - Environment Interaction:\n",
    "The environment returns the next observation, reward, and termination status.\n",
    "    - Transition Storage:\n",
    "The experience  $(s, a, s{\\prime}, r)$  is stored in the replay memory.\n",
    "    - Optimization:\n",
    "The optimize_model method updates the network parameters.\n",
    "- Target Update:\n",
    "The soft_update method softly updates the target network.\n",
    "- Episode Termination:\n",
    "The loop breaks if the environment signals termination.\n",
    "3. Episode Reward Tracking:\n",
    "Total reward per episode is stored and plotted.\n",
    "4. Completion:\n",
    "After all episodes, the final reward plot is displayed.\n",
    "\n",
    "**Mathematical Perspective:**\n",
    "\n",
    "For each time step  $t$  in an episode:\n",
    "\n",
    "- The agent observes state  $s_t$  and selects action  $a_t$  using the \\epsilon-greedy policy.\n",
    "- It receives reward  $r_t$  and observes new state  $s_{t+1}$.\n",
    "- The optimization step minimizes the loss:\n",
    "\n",
    "$$\n",
    "L_t(\\theta) = \\left( r_t + \\gamma \\max_{a{\\prime}} Q(s_{t+1}, a{\\prime}; \\theta^{-}) - Q(s_t, a_t; \\theta) \\right)^2\n",
    "$$\n",
    "\n",
    "- The network parameters are updated with gradient descent, while the target network is softly updated:\n",
    "\n",
    "$$\n",
    "\\theta^{-} \\leftarrow \\tau \\theta + (1 - \\tau) \\theta^{-}\n",
    "$$\n",
    "\n",
    "**PyTorch Specifics:**\n",
    "\n",
    "- All tensors are managed on the specified device (e.g., GPU for faster computation).\n",
    "- The optimizer `optim.AdamW` is used to update the network parameters.\n",
    "- States are represented as tensors to enable GPU acceleration and efficient numerical computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class HyperParams:\n",
    "    BATCH_SIZE: int = 512\n",
    "    GAMMA: float = 0.99\n",
    "    EPS_START: float = 0.9\n",
    "    EPS_END: float = 0.05\n",
    "    EPS_DECAY: int = 1000\n",
    "    TAU: float = 0.005\n",
    "    LR: float = 1e-4\n",
    "\n",
    "\n",
    "params = HyperParams()\n",
    "env = DMControlCartPoleWrapper(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "# Number of discrete actions\n",
    "n_actions = env.action_space_n\n",
    "# Dimensionality of the observations\n",
    "n_observations = env.obs_size\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "from src.dqn import DQNTrainer\n",
    "\n",
    "trainer = DQNTrainer(env, memory, device, params, num_episodes=500)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Expectations\n",
    "\n",
    "If you have implemented the DQN algorithm correctly, you should observe your reward curve increasing over time. The agent should learn to swingup and balance the pole on the cart, achieving a high reward in each episode. You should observe at least more than 200 rewards in the majority of episodes after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Agent\n",
    "In order to run the following cell, which generates a rollout and render the environment, you need to install the `ffmpeg` package. You can install it by running the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "sudo apt-get install ffmpeg\n",
    "```\n",
    "\n",
    "On macOS, you can install it using `brew`:\n",
    "\n",
    "```bash\n",
    "brew install ffmpeg\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import display_video\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "env = DMControlCartPoleWrapper(domain_name=\"cartpole\", task_name=\"swingup\")\n",
    "\n",
    "frames = []\n",
    "obs, _ = env.reset()\n",
    "state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    # Capture a frame using dm_control's render\n",
    "    frame = env.env.physics.render(height=400, width=400, camera_id=1)\n",
    "    frames.append(frame)\n",
    "\n",
    "    # Select the best action using policy_net\n",
    "    with torch.no_grad():\n",
    "        action = trainer.policy_net(state).max(1)[1].item()\n",
    "\n",
    "    observation, reward, terminated, truncated, _ = env.step(action)\n",
    "    if not (terminated or truncated):\n",
    "        state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(\n",
    "            0\n",
    "        )\n",
    "\n",
    "display_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission:\n",
    "\n",
    "For this part, you need to submit your implementation of the DQN algorithm for the Cartpole Balance task. You need to implement the `dqn.py` file alongside with the `rewards_plot_dqn.png` file generated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
