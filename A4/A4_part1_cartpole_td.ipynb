{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4 Part 1: MuJoCo CartPole - Reinforcement Learning Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In this section, you are going to set up the MuJoCo environment to do physics-based reinforcement learning. Make sure you have the required packages installed as listed in the `environment.yml` file.\n",
    "\n",
    "## MuJoCo Installation\n",
    "\n",
    "MuJoCo now is open-sourced project and can be installed via the pip package. You can install it by running the following command:\n",
    "\n",
    "```bash\n",
    "pip install mujoco\n",
    "```\n",
    "\n",
    "## `dm_control` (deep-mind control) Installation\n",
    "\n",
    "`dm_control` is a package that provides a suite of continuous control tasks. You can install it by running the following command:\n",
    "\n",
    "```bash\n",
    "pip install dm_control\n",
    "```\n",
    "\n",
    "Make sure you have `cmake` installed on your system.\n",
    "\n",
    "## `ffmpeg` Installation\n",
    "\n",
    "`ffmpeg` is a package that is used to render the video of the environment. You can install it by running the following command:\n",
    "\n",
    "If you are on linux, you can install it by running the following command:\n",
    "\n",
    "```bash\n",
    "sudo apt-get install ffmpeg \n",
    "```\n",
    "\n",
    "If you are on macOS, you can install it by running the following command, make sure you have `brew` installed:\n",
    "\n",
    "```bash\n",
    "brew install ffmpeg\n",
    "```\n",
    "\n",
    "If you are on Windows, you can install it by running the following command:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge ffmpeg\n",
    "```\n",
    "\n",
    "### `mujoco_tutorial.ipynb`\n",
    "\n",
    "Before starting this part of the assignment, please make sure you have completed the `mujoco_tutorial.ipynb` notebook. This notebook will help you understand the basics of the MuJoCo environment and how to interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# The basic mujoco wrapper.\n",
    "from dm_control import mujoco\n",
    "from dm_control import suite\n",
    "from tqdm import tqdm\n",
    "\n",
    "#@title Other imports and helper functions\n",
    "\n",
    "# General\n",
    "import copy\n",
    "import os\n",
    "import itertools\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "# Graphics-related\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import PIL.Image\n",
    "# Internal loading of video libraries.\n",
    "\n",
    "# Use svg backend for figure rendering\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# Font sizes\n",
    "SMALL_SIZE = 8\n",
    "MEDIUM_SIZE = 10\n",
    "BIGGER_SIZE = 12\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "from src.utils import display_video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole Balance & Swingup Tasks\n",
    "\n",
    "In the following section, we will look at the Cartpole Balance and Swingup tasks. The Cartpole Balance task is a classic control problem where the goal is to balance a pole on a cart. The Cartpole Swingup task is a more challenging version of the Cartpole Balance task where the goal is to swing the pole up and balance it on the cart. This will be broken into two parts.\n",
    "\n",
    "1. **Part 1:** Implement TD-Learning for both Cartpole Balance Task and Cartpole Swingup Task.\n",
    "   - Define the TD-Learning algorithm function.\n",
    "   - Initialize necessary parameters such as learning rate and discount factor.\n",
    "   - Implement the training loop to update values using the TD-Learning algorithm.\n",
    "\n",
    "2. **Part 2:** Explore different quantization hyperparameters to optimize the performance of the TD-Learning algorithm.\n",
    "    - Implement the training loop to update values using the TD-Learning algorithm.\n",
    "    - Explore different quantization hyperparameters to optimize the performance of the TD-Learning algorithm.\n",
    "    -Leaderboard Submission\n",
    "        - Submit your best `q_table` alongside with your hyperparameter of the quantization to gradescope, we will evaluate your submission based on the performance of the `q_table` and rank your performance. The top 10 % of the submission will receive 1pts extra credit on this assignment.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement TD-Learning for Cartpole Balance Task\n",
    "\n",
    "In this sub-part, you will implement the TD-Learning algorithm for the Cartpole Balance task. The Cartpole Balance task is a classic control problem where the goal is to balance a pole on a cart. The state space consists of the cart's position, the cart's velocity, the pole's angle, and the pole's angular velocity.\n",
    "\n",
    "### Take a look at the environment\n",
    "\n",
    "The following cell runs the Cartpole Balance task for a single episode and renders the environment. Below, you will implement the TD-Learning algorithm to interact with this environment and learn a policy for balancing the pole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random policy for cartpole balance task\n",
    "\n",
    "# Load the environment\n",
    "random_state = np.random.RandomState(42)\n",
    "env = suite.load(\"cartpole\", \"balance\", task_kwargs={\"random\": random_state})\n",
    "\n",
    "# Simulate episode with random actions\n",
    "duration = 5  # Seconds\n",
    "frames = []\n",
    "ticks = []\n",
    "rewards_balance = []\n",
    "observations = []\n",
    "\n",
    "spec = env.action_spec()\n",
    "time_step = env.reset()\n",
    "\n",
    "# Simulate the environment (environment loop)\n",
    "while env.physics.data.time < duration:\n",
    "    action = random_state.uniform(spec.minimum, spec.maximum, spec.shape)\n",
    "    time_step = env.step(action)\n",
    "    camera0 = env.physics.render(camera_id=0, height=400, width=400)\n",
    "    camera1 = env.physics.render(camera_id=1, height=400, width=400)\n",
    "    frames.append(np.hstack((camera0, camera1)))\n",
    "    rewards_balance.append(time_step.reward)\n",
    "    observations.append(copy.deepcopy(time_step.observation))\n",
    "    ticks.append(env.physics.data.time)\n",
    "\n",
    "\n",
    "html_video = display_video(frames, framerate=1.0 / env.control_timestep())\n",
    "\n",
    "html_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random policy for cartpole swingup task\n",
    "\n",
    "# Load the environment\n",
    "random_state = np.random.RandomState(42)\n",
    "env = suite.load(\"cartpole\", \"swingup\", task_kwargs={\"random\": random_state})\n",
    "\n",
    "# Simulate episode with random actions\n",
    "duration = 5  # Seconds\n",
    "frames = []\n",
    "ticks = []\n",
    "rewards_balance = []\n",
    "observations = []\n",
    "\n",
    "spec = env.action_spec()\n",
    "time_step = env.reset()\n",
    "\n",
    "# Simulate the environment (environment loop)\n",
    "while env.physics.data.time < duration:\n",
    "    action = random_state.uniform(spec.minimum, spec.maximum, spec.shape)\n",
    "    time_step = env.step(action)\n",
    "    camera0 = env.physics.render(camera_id=0, height=400, width=400)\n",
    "    camera1 = env.physics.render(camera_id=1, height=400, width=400)\n",
    "    frames.append(np.hstack((camera0, camera1)))\n",
    "    rewards_balance.append(time_step.reward)\n",
    "    observations.append(copy.deepcopy(time_step.observation))\n",
    "    ticks.append(env.physics.data.time)\n",
    "\n",
    "\n",
    "html_video = display_video(frames, framerate=1.0 / env.control_timestep())\n",
    "\n",
    "html_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the cartpole environment rendering, but with random actions. The pole should fall over quickly. This is not very intelligent right? Luckily, we can do a lot better with reinforcement learning! One of the simplest algorithms is TD-Learning, which we will implement in this sub-part. Before doing that, let's do more analysis on the episode we just simulated.\n",
    "\n",
    "Since the whole environment is simulated in MuJoCo, this means that we get access to every single state of the environment, including the observations and the rewards throughout the episode. The following cell will plot the observations and rewards throughout the episode. Later, you will use this graph to understand the MDP of the Cartpole Balance task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_episode_stats(observations: list, rewards: list, ticks: list):\n",
    "    \"\"\"\n",
    "    Plot the episode statistics based on the observations\n",
    "\n",
    "    Args:\n",
    "        observations (list): List of observations from the environment\n",
    "    \"\"\"\n",
    "    num_sensors = len(observations[0])\n",
    "\n",
    "    _, ax = plt.subplots(1 + num_sensors, 1, sharex=True, figsize=(4, 8))\n",
    "    ax[0].plot(ticks, rewards)\n",
    "    ax[0].set_ylabel(\"reward\")\n",
    "    ax[-1].set_xlabel(\"time\")\n",
    "\n",
    "    for i, key in enumerate(observations[0]):\n",
    "        data = np.asarray([observations[j][key] for j in range(len(observations))])\n",
    "        ax[i + 1].plot(ticks, data, label=key)\n",
    "        ax[i + 1].set_ylabel(key)\n",
    "    plt.show()\n",
    "\n",
    "plot_episode_stats(observations, rewards_balance, ticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: TD Learning for Cartpole with quantization of state space and action space\n",
    "\n",
    "In this task, we will implement TD Learning for the Cartpole environment. One way to handle continuous state and action spaces is by discretizing them into bins. This process, known as quantization, allows us to apply discrete reinforcement learning algorithms to continuous tasks. We will define the state and action bins, initialize the Q-table, train the agent using the TD Learning algorithm, and visualize the learning progress by plotting the total rewards obtained in each episode.\n",
    "\n",
    "### Define the state and action bins\n",
    "\n",
    "Since the cartpole environment receives a continuous state space, we need to discretize it into bins to make the state space and action space discrete. We will define the state and action bins for the Cartpole environment. The state bins will be used to discretize the continuous state space, while the action bins will be used to discretize the continuous action space. By doing this, we can construct the Q-table for the task.\n",
    "\n",
    "Before doing that, let's take a look at the state and action spaces of the Cartpole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the action spec\n",
    "spec = env.action_spec()\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the first observation, action, and reward\n",
    "observations[-1], action[0], rewards_balance[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the states representation, we have 5 dimensional observations, three for the position, and two for velocity. For action space, we have 1-dimensional action space, which is the force applied to the cart. Let's start to quantize (discretize) the state and action space.\n",
    "\n",
    "For the discretization, we will define a sequences of bins for each dimension of the state and action space. We will use the `np.linspace` function to create these bins. The number of bins for each dimension is a hyperparameter that you can tune to optimize the performance of the TD Learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter of Discretization\n",
    "\n",
    "The following bin configs are provided as a starting point. The algorithm might learn something but the quantization is not optimal. You can change these values to optimize the performance of the TD Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom bin configurations for each observation dimension.\n",
    "# For each observation variable, specify a list of (min, max, bin_nums)\n",
    "# In this example, we assume:\n",
    "#   - 'position' has 3 dimensions and we use BIGGER_SIZE bins for each.\n",
    "#   - 'velocity' has 2 dimensions and we use MEDIUM_SIZE bins for each.\n",
    "# Define custom bin configurations for each observation dimension individually.\n",
    "# You can adjust the range and bin count for each dimension as needed.\n",
    "pos_bins_config = [\n",
    "    (-1.0, 1.0, 2),  # Config for the first position component\n",
    "    (-1.0, 1.0, 2),  # Config for the second position component\n",
    "    (-2.0, 2.0, 5),  # Config for the third position component\n",
    "]\n",
    "\n",
    "vel_bins_config = [\n",
    "    (-1.0, 1.0, 2),  # Config for the first velocity component\n",
    "    (-10.0, 10.0, 5),  # Config for the second velocity component\n",
    "]\n",
    "\n",
    "obs_bins_config = {\"position\": pos_bins_config, \"velocity\": vel_bins_config}\n",
    "\n",
    "# Create a list of bins arrays corresponding to each observation dimension.\n",
    "state_bins = {\n",
    "    key: [\n",
    "        np.linspace(min_val, max_val, bin_num) for min_val, max_val, bin_num in config\n",
    "    ]\n",
    "    for key, config in obs_bins_config.items()\n",
    "}\n",
    "\n",
    "action_bins_config = (-1, 1, 4)\n",
    "\n",
    "action_bins = np.linspace(*action_bins_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after we define the bins, we now can start to quantize the states. Implement the functions that takes the continuous state and returns the quantized state in `src/discretize.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.discretize import quantize_state\n",
    "\n",
    "\n",
    "time_step = env.reset()\n",
    "obs = time_step.observation\n",
    "\n",
    "quantized_state = quantize_state(obs, state_bins)\n",
    "print(\"Quantized State:\", quantized_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now we can tackle the continuous control problem with discrete reinforcement learning algorithms. Next, based on the quantized state and action spaces, we will initialize the Q-table for the Cartpole environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.td import initialize_q_table\n",
    "\n",
    "q_table_balance = initialize_q_table(state_bins, action_bins)\n",
    "print(\"Initialized Q-Table shape:\", q_table_balance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-table shape will be dependent on how you setup the quantization. If you quantize the space with more bins, your q-table will be larger. Let's take a look at the total number of parameter in the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of Q-values:\", np.prod(q_table_balance.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be important when you explore the hyperparameter of the discretization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's implement the TD learning algorithm based on the quantization and the q-table we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.td import td_learning\n",
    "\n",
    "# Define the environment and parameters\n",
    "env = suite.load(\"cartpole\", \"balance\")\n",
    "num_episodes = 1000  # hyperparameter\n",
    "alpha = 0.1  # hyperparameter\n",
    "gamma = 0.99  # hyperparameter\n",
    "epsilon = 0.1  # hyperparameter\n",
    "\n",
    "# Train the agent\n",
    "q_table_balance, rewards_balance = td_learning(\n",
    "    env, num_episodes, alpha, gamma, epsilon, state_bins, action_bins\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after training, we can plot the reward progression of the learner as it learns to balance the pole on the cart.\n",
    "\n",
    "**Submission:** Submit the following reward progression plot `td-balance.png` to the Gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rewards\n",
    "# if it is weird looking, run it twice\n",
    "plt.plot(rewards_balance)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"TD Learning for Balance Cartpole\")\n",
    "# save the figure for submission\n",
    "plt.savefig(\"td-balance.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the agent, we should take a look at the q-table to see how the agent has learned to balance the pole on the cart. The following cell will displace the total number of elements in the q-table and the total number of non-zero elements in the q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_q_table(q_table: np.ndarray):\n",
    "    \"\"\"\n",
    "    Inspect the Q-table for characteristics of the learned values.\n",
    "\n",
    "    Args:\n",
    "        q_table (np.ndarray): Q-values for state-action pairs.\n",
    "    \"\"\"\n",
    "    total_elements = np.prod(q_table.shape)\n",
    "    non_zero_elements = np.sum(np.isclose(q_table, 0))\n",
    "    total_non_zero_elements = total_elements - non_zero_elements\n",
    "\n",
    "    print(f\"Total number of Q-values: {total_elements}\")\n",
    "    print(f\"Number of non-zero Q-values: {total_non_zero_elements}\")\n",
    "    print(\n",
    "        f\"Percentage of non-zero Q-values: {100 * total_non_zero_elements / total_elements:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "inspect_q_table(q_table_balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is very sparse! To improve the performance of your algorithm, you should explore how to better discretize the space in order to have a better representation of the states. You will do that in the next part where you are going to optimize the performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.td import greedy_policy\n",
    "from src.utils import run_episode\n",
    "\n",
    "env = suite.load(\"cartpole\", \"balance\")\n",
    "\n",
    "# Define the greedy policy\n",
    "greedy_policy_fn = greedy_policy(q_table_balance)\n",
    "\n",
    "# Example usage: get the action for a given state\n",
    "example_state = quantize_state(env.reset().observation, state_bins)\n",
    "action = greedy_policy_fn(example_state)\n",
    "print(f\"Greedy action for the example state: {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an episode using the greedy policy\n",
    "total_reward, frames, stats = run_episode(\n",
    "    env, greedy_policy_fn, state_bins, action_bins\n",
    ")\n",
    "\n",
    "plot_episode_stats(*stats)\n",
    "\n",
    "# Display the video of the episode\n",
    "html_video = display_video(frames, framerate=50)\n",
    "print(f\"Total reward received during the episode: {total_reward}\")\n",
    "html_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-part 2: Train the cartpole swing up tasks\n",
    "\n",
    "In previous section, you have trained the balance task. The swing up tasks is more difficult than the balance task since the cart needs to first swing up the pole, and then keep it balance. Let's train the swing up task with the same TD learning algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.td import td_learning\n",
    "\n",
    "# Define the environment and parameters\n",
    "env = suite.load(\"cartpole\", \"swingup\")\n",
    "num_episodes = 1000  # hyperparameter\n",
    "alpha = 0.1  # hyperparameter\n",
    "gamma = 0.99  # hyperparameter\n",
    "epsilon = 0.1  # hyperparameter\n",
    "\n",
    "# Train the agent\n",
    "q_table_swingup, rewards_swingup = td_learning(\n",
    "    env, num_episodes, alpha, gamma, epsilon, state_bins, action_bins\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission:** Submit the following reward progression plot `td-swingup.png` to the Gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rewards\n",
    "\n",
    "### if it is weird looking, run it twice ###\n",
    "plt.plot(rewards_swingup)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"TD Learning for Swingup Cartpole\")\n",
    "\n",
    "# save the figure for submission\n",
    "plt.savefig(\"td-swingup.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_q_table(q_table_swingup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.load(\"cartpole\", \"swingup\")\n",
    "\n",
    "# Define the greedy policy\n",
    "greedy_policy_fn = greedy_policy(q_table_swingup)\n",
    "\n",
    "# Run an episode using the greedy policy\n",
    "total_reward, frames, stats = run_episode(\n",
    "    env, greedy_policy_fn, state_bins, action_bins\n",
    ")\n",
    "\n",
    "\n",
    "plot_episode_stats(*stats)\n",
    "\n",
    "# Display the video of the episode\n",
    "html_video = display_video(frames, framerate=50)\n",
    "print(f\"Total reward received during the episode: {total_reward}\")\n",
    "html_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Explore Continual Learning between Tasks\n",
    "\n",
    "In the previous section, you have trained the balance task and the swing up task separately. In this section, you can explore how to train the balance task first, and then train the swing up task on top of the balance task. This is a form of continual learning where the agent learns to balance the pole first, and then learns to swing up the pole. You can explore how to transfer the knowledge from the balance task to the swing up task to improve the performance of the agent.\n",
    "\n",
    "One way to transfer knowledge from one task to another is to initialize the Q-table of the swing up task with the Q-table learned from the previous task of balance. You can explore how to initialize the Q-table with the Q-table learned from the balance task to improve the performance of the agent on the swing up task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment and parameters\n",
    "env = suite.load(\"cartpole\", \"swingup\")\n",
    "num_episodes = 1000  # hyperparameter\n",
    "alpha = 0.1  # hyperparameter\n",
    "gamma = 0.99  # hyperparameter\n",
    "epsilon = 0.1  # hyperparameter\n",
    "\n",
    "# Train the agent with initial knowledge from the balance task\n",
    "q_table_swingup_transfer, rewards_swingup_transfer = td_learning(\n",
    "    env,\n",
    "    num_episodes,\n",
    "    alpha,\n",
    "    gamma,\n",
    "    epsilon,\n",
    "    state_bins,\n",
    "    action_bins,\n",
    "    q_table=q_table_balance,  # reusing the q_table from the balance task\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rewards\n",
    "# if it is weird looking, run it twice\n",
    "plt.plot(rewards_swingup_transfer)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"TD Learning for Transfer Swingup Cartpole from Balance\")\n",
    "\n",
    "plt.savefig(\"td_learning_transfer_swingup_cartpole.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_q_table(q_table_swingup_transfer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.load(\"cartpole\", \"swingup\")\n",
    "\n",
    "# Define the greedy policy\n",
    "greedy_policy_fn = greedy_policy(q_table_swingup_transfer)\n",
    "\n",
    "# Run an episode using the greedy policy\n",
    "total_reward, frames, stats = run_episode(\n",
    "    env, greedy_policy_fn, state_bins, action_bins\n",
    ")\n",
    "\n",
    "\n",
    "plot_episode_stats(*stats)\n",
    "\n",
    "# Display the video of the episode\n",
    "html_video = display_video(frames, framerate=50)\n",
    "print(f\"Total reward received during the episode: {total_reward}\")\n",
    "html_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Explore different quantization and training hyperparameters\n",
    "\n",
    "In this part, you will explore different quantization and training hyperparameters to optimize the performance of the TD-Learning algorithm for the Cartpole Balance and Swingup tasks. You will use the training algorithm we implemented in the previous part, and explore different hyperparameters to optimize the performance of the agent.\n",
    "\n",
    "We will benchmark the performance of the agent based on the total rewards obtained in each episode. You will explore different hyperparameters such as the number of bins for the state and action spaces, the learning rate, and the discount factor. You will train the agent with different hyperparameters and plot the total rewards obtained in each episode to analyze the performance of the agent.\n",
    "\n",
    "For submission, you need to submit two pickle files that contain the q-table and the bins for the balance and swing-up tasks. We will evaluate your submission based on the performance of the q-table and the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom bin configurations for each observation dimension.\n",
    "# For each observation variable, specify a list of (min, max, bin_nums)\n",
    "# In this example, we assume:\n",
    "#   - 'position' has 3 dimensions and we use BIGGER_SIZE bins for each.\n",
    "#   - 'velocity' has 2 dimensions and we use MEDIUM_SIZE bins for each.\n",
    "# Define custom bin configurations for each observation dimension individually.\n",
    "# You can adjust the range and bin count for each dimension as needed.\n",
    "pos_bins_config = [\n",
    "    (-2.0, 2.0, 2),  # Config for the first position component\n",
    "    (-2.0, 2.0, 2),  # Config for the second position component\n",
    "    (-2.0, 2.0, 2),  # Config for the third position component\n",
    "]\n",
    "\n",
    "vel_bins_config = [\n",
    "    (-3.0, 3.0, 2),  # Config for the first velocity component\n",
    "    (-3.0, 3.0, 5),  # Config for the second velocity component\n",
    "]\n",
    "\n",
    "obs_bins_config = {\"position\": pos_bins_config, \"velocity\": vel_bins_config}\n",
    "\n",
    "# Create a list of bins arrays corresponding to each observation dimension.\n",
    "state_bins = {\n",
    "    key: [\n",
    "        np.linspace(min_val, max_val, bin_num) for min_val, max_val, bin_num in config\n",
    "    ]\n",
    "    for key, config in obs_bins_config.items()\n",
    "}\n",
    "\n",
    "action_bins_config = (-1, 1, 5)\n",
    "\n",
    "action_bins = np.linspace(*action_bins_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: call your q learning here and save to the q_table variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaderboard Submission\n",
    "\n",
    "Save your variable of the q-table and the bins for the balance and swing-up tasks in a pickle file in the following cell. Feel free to create as many cell as you need to save the variables. We will evaluate your submission based on the performance of the q-table and the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "q_table_balance = ...\n",
    "state_bins_balance, action_bins_balance = ..., ...\n",
    "\n",
    "with open(\"q_table_balance.pkl\", \"wb\") as f:\n",
    "    pickle.dump((q_table_balance, state_bins_balance, action_bins_balance), f)\n",
    "\n",
    "q_table_swingup = ...\n",
    "state_bins_swingup, action_bins_swingup = ..., ...\n",
    "\n",
    "with open(\"q_table_swingup.pkl\", \"wb\") as f:\n",
    "    pickle.dump((q_table_swingup, state_bins_swingup, action_bins_swingup), f)\n",
    "\n",
    "# submit those two file to gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: DQN for Cartpole Balance Task\n",
    "\n",
    "In this part, you will implement the Deep Q-Network (DQN) algorithm for the Cartpole Balance task. The DQN algorithm uses a neural network to approximate the Q-function, which allows us to learn a policy for balancing the pole on the cart. You will implement the DQN algorithm, train the agent using the Cartpole Balance task, and visualize the learning progress by plotting the total rewards obtained in each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)\n",
    "plt.title(\"DQN on Cartpole Balance (MPS)\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.load(\"cartpole\", \"balance\")\n",
    "time_step = env.reset()\n",
    "frames = []\n",
    "total_reward = 0\n",
    "\n",
    "while not time_step.last():\n",
    "    state = collect_state(time_step.observation)\n",
    "    state_t = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state_t)\n",
    "        action_idx = int(torch.argmax(q_values))\n",
    "    action_val = ACTION_LIST[action_idx].reshape((1,))\n",
    "    # print(action_val)\n",
    "    time_step = env.step(action_val)\n",
    "    total_reward += time_step.reward or 0.0\n",
    "\n",
    "    # record frames from two cameras\n",
    "    camera0 = env.physics.render(camera_id=0, height=400, width=400)\n",
    "    camera1 = env.physics.render(camera_id=1, height=400, width=400)\n",
    "    frames.append(np.hstack((camera0, camera1)))\n",
    "\n",
    "html_video = display_video(frames, framerate=50)\n",
    "print(f\"Total reward: {total_reward}\")\n",
    "html_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "Required files for submission:\n",
    "\n",
    "1. `td-balance.png`: Reward progression plot for the Cartpole Balance task.\n",
    "2. `td-swingup.png`: Reward progression plot for the Cartpole Swingup task.\n",
    "3. `q_table_balance.pkl`: Pickle file containing the q-table and bins for the Cartpole Balance task.\n",
    "4. `q_table_swingup.pkl`: Pickle file containing the q-table and bins for the Cartpole Swingup task.\n",
    "5. `discretize.py`: Python file containing the implementation of the discretize function.\n",
    "6. `td.py`: Python file containing the implementation of the TD Learning algorithm.\n",
    "\n",
    "\n",
    "Submit the above files to the Gradescope. We will evaluate your submission based on the performance of the q-table and the bins. The top 10% of the submission will receive 1 point extra credit on this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
